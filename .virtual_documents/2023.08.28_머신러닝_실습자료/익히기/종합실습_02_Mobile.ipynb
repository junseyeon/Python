











# 라이브러리 불러오기
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings

warnings.filterwarnings(action='ignore')
%config InlineBackend.figure_format = 'retina'


# 데이터 불러오기
path = 'https://raw.githubusercontent.com/jangrae/csv/master/mobile_cust_churn.csv'
data = pd.read_csv(path)
data['CHURN'] = data['CHURN'].map({'STAY':0, 'LEAVE': 1})





# 데이터 살펴보기
data.head()





# 기술통계 확인
data.describe()








# 제거 대상: id
data.drop('id', axis=1 ,inplace=True)
data
# 변수 제거


# 확인






# Target 설정

target = 'CHURN'
# 데이터 분리
x = data.drop(target, axis=1)
y = data[target]






# 가변수화 대상: REPORTED_SATISFACTION, REPORTED_USAGE_LEVEL, CONSIDERING_CHANGE_OF_PLAN
col = ['REPORTED_SATISFACTION', 'REPORTED_USAGE_LEVEL', 'CONSIDERING_CHANGE_OF_PLAN']

# 가변수화
x = pd.get_dummies(x, columns=col, drop_first=True)

# 확인
x.head()





# 모듈 불러오기
from sklearn.model_selection import train_test_split

# 7:3으로 분리
x_train, x_test , y_train, y_test = train_test_split(x, y , test_size=0.3, random_state=1)





# 모듈 불러오기
from sklearn.preprocessing import MinMaxScaler

# 정규화
scaler = MinMaxScaler()
x_train_s = scaler.fit_transform(x_train)
x_test_s = scaler.transform(x_test)








# xgboost 설치
# pip install xgboost


# lightgbm 설치
# pip install lightgbm





# 불러오기
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

from sklearn.model_selection import cross_val_score, GridSearchCV
from sklearn.metrics import confusion_matrix, classification_report





# 선언하기
model = KNeighborsClassifier(n_neighbors=5)


# 성능예측
cv_score = cross_val_score(model, x_train, y_train, cv=5)


# 결과확인
print(cv_score,cv_score.mean(), cv_score.std())


# 결과수집
result = {}
result['KNN'] = cv_score.mean()





# 선언하기
model = DecisionTreeClassifier(random_state=1, max_depth=5)


# 성능예측
cv_score = cross_val_score(model, x_train, y_train, cv=5)


# 결과확인
print(cv_score,cv_score.mean(), cv_score.std())


# 결과수집
result['Decision Tree'] = cv_score.mean()





# 선언하기
model = LogisticRegression()


# 성능예측
cv_score = cross_val_score(model, x_train, y_train, cv=5)


# 결과확인
print(cv_score,cv_score.mean(), cv_score.std())


# 결과수집
result['Logistic Regression'] = cv_score.mean()





# 선언하기
model = RandomForestClassifier(max_depth=5, random_state=1)


# 성능예측
cv_score = cross_val_score(model, x_train, y_train, cv=5)


# 결과확인
print(cv_score,cv_score.mean(), cv_score.std())


# 결과수집
result['Random Forest'] = cv_score.mean()





# 선언하기
model = XGBClassifier(max_depth=5,random_state=1)


# 성능예측
cv_score = cross_val_score(model, x_train, y_train, cv=5)


# 결과확인
print(cv_score,cv_score.mean(), cv_score.std())


# 결과수집
result['XGBoost'] = cv_score.mean()





# 선언하기
model = LGBMClassifier(max_depth=5,random_state=1, verbose=-100)


# 성능예측
cv_score = cross_val_score(model, x_train, y_train, cv=5)


# 결과확인
cross_val_score(model, x_train, y_train, cv=5)


# 결과수집
result['LightGBM'] = cv_score.mean()





# 성능 비교
print('=' * 40)
for m_name, score in result.items():
    print(m_name, score.round(3))
print('=' * 40)


# 성능 시각화 비교
plt.barh(list(result.keys()), result.values())
plt.show()





# 기본 모델 선언
model = LGBMClassifier(max_depth=5,random_state=1, verbose=-100)

# 파라미터 지정
  # max_depth: range(1, 21)
param = {'max_depth':range(1,21)}

# 모델 선언

model_tune = GridSearchCV( model, param, cv=3 )





# 학습하기(많은 시간이 소요될 수 있음)
model_tune.fit(x_train, y_train)


# 최적 파라미터, 예측 최고 성능
print(model_tune.cv_results_['mean_test_score'])
print('-' * 80)
print('최적파라미터:', model_tune.best_params_)
print('-' * 80)
print('최고성능:', model_tune.best_score_)



# 변수 중요도 시각화
print(list(x_test))
model_tune.best_estimator_.feature_importances_






# 예측하기
y_pred = model_tune.predict(x_test)


# 성능평가
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))



